{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PnZwmUcE9vLT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnZwmUcE9vLT",
        "outputId": "054d6dc6-32b7-41d6-d88e-3a71f421d718"
      },
      "outputs": [],
      "source": [
        "colab = False\n",
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438889f7",
      "metadata": {
        "id": "438889f7"
      },
      "source": [
        "## **Week 4: From MLPs to Convolutional Neural Networks**\n",
        "### José Manuel López, Alex Martín, Marcos V. Conde\n",
        "\n",
        "#### <ins>Understanding Layer Manipulation</ins>\n",
        "1. Check existing architectures\n",
        "2. Set a new model from an existing architecture\n",
        "3. Apply model to a small dataset\n",
        "\n",
        "#### <ins>Deal with dataset loading</ins>\n",
        "4. Introduce and evaluate Data Augmentation Impact\n",
        "\n",
        "#### <ins>Hyperparametrs optimization</ins>\n",
        "4. Introduce and evaluate the usage of dropout, batch normalization, ...\n",
        "5. Apply random search on model hyperparametes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "677052bb",
      "metadata": {
        "id": "677052bb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.util import deprecation\n",
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "from tensorflow.python.client import device_lib \n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import os\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab260a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "#reset Keras Session\n",
        "def reset_keras():\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "    tf.compat.v1.keras.backend.clear_session()\n",
        "    sess.close()\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "    \n",
        "    del model\n",
        "\n",
        "    # use the same config as you used to create the session\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "    config.gpu_options.visible_device_list = \"0\"\n",
        "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
        "    \n",
        "\n",
        "reset_keras()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5e2f0d",
      "metadata": {
        "id": "dd5e2f0d"
      },
      "source": [
        "Tensorflow allocates all VRAM without this.\n",
        "Additionally we want to enable mixed precision aka use TensorCores if our GPU is capable to speedup the computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f90b997",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f90b997",
        "outputId": "cea2628b-87ce-4100-9d10-89e78a37aaff"
      },
      "outputs": [],
      "source": [
        "# Disable Warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# VRAM broke without this\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "\n",
        "try:\n",
        "    # print model to see if it's compatible with Mixed Precision\n",
        "    print(device_lib.list_local_devices()[1].physical_device_desc)\n",
        "    # Change to TF16 mixed precision\n",
        "    policy = mixed_precision.Policy('mixed_float16')\n",
        "    mixed_precision.set_policy(policy)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(gpus[0])\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f0c4f8",
      "metadata": {
        "id": "91f0c4f8"
      },
      "source": [
        "### Loading Datasets\n",
        "\n",
        "We are going to define variables related to dataset and load data into generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8c8e57",
      "metadata": {
        "id": "3a8c8e57"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dedLxP71JOue",
      "metadata": {
        "id": "dedLxP71JOue"
      },
      "outputs": [],
      "source": [
        "def preprocess_input_manual(x, dim_ordering='default'):\n",
        "    if dim_ordering == 'default':\n",
        "        dim_ordering = K.image_data_format()\n",
        "    assert dim_ordering in {'channels_first', 'channels_last'}\n",
        "\n",
        "    if dim_ordering == 'channels_first':\n",
        "        # 'RGB'->'BGR'\n",
        "        x = x[ ::-1, :, :]\n",
        "        # Zero-center by mean pixel\n",
        "        x[ 0, :, :] -= 103.939\n",
        "        x[ 1, :, :] -= 116.779\n",
        "        x[ 2, :, :] -= 123.68\n",
        "    else:\n",
        "        # 'RGB'->'BGR'\n",
        "        x = x[:, :, ::-1]\n",
        "        # Zero-center by mean pixel\n",
        "        x[:, :, 0] -= 103.939\n",
        "        x[:, :, 1] -= 116.779\n",
        "        x[:, :, 2] -= 123.68\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b27ff6fa",
      "metadata": {
        "id": "b27ff6fa"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "RANDOM_SEED = 42\n",
        "validation_samples = 2288\n",
        "CLASSES = ['coast','forest','highway','inside_city','mountain','Opencountry','street','tallbuilding']\n",
        "def get_dataset(path, batch_size=32):\n",
        "    print(\"Loading: {}\".format(path))\n",
        "    datagen = ImageDataGenerator(featurewise_center=False,\n",
        "              samplewise_center=False,\n",
        "              featurewise_std_normalization=False,\n",
        "              samplewise_std_normalization=False,\n",
        "              preprocessing_function=preprocess_input_manual,\n",
        "              rotation_range=0.,\n",
        "              width_shift_range=0.,\n",
        "              height_shift_range=0.,\n",
        "              shear_range=0.,\n",
        "              zoom_range=0.,\n",
        "              channel_shift_range=0.,\n",
        "              fill_mode='nearest',\n",
        "              cval=0.,\n",
        "              horizontal_flip=False,\n",
        "              vertical_flip=False,\n",
        "              rescale=None)\n",
        "\n",
        "    train_loader = datagen.flow_from_directory(\n",
        "                path+'/train',  \n",
        "                target_size=(IMG_SIZE, IMG_SIZE), \n",
        "                batch_size=BATCH_SIZE,\n",
        "                classes = CLASSES,\n",
        "                class_mode='categorical') \n",
        "\n",
        "    test_loader = datagen.flow_from_directory(\n",
        "            path+'/test',\n",
        "            target_size=(IMG_SIZE, IMG_SIZE),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            classes = CLASSES,\n",
        "            class_mode='categorical')\n",
        "    \n",
        "\n",
        "    examples = enumerate(test_loader)\n",
        "    batch_idx, (example_data, example_targets) = next(examples)\n",
        "    print(\"Data Shape: {}\".format(example_data.shape))\n",
        "    print(\"---\"*30)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b067dfaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "b067dfaf",
        "outputId": "00cfe3f5-839d-49ad-c952-936bebc8d395"
      },
      "outputs": [],
      "source": [
        "\n",
        "if colab:\n",
        "    DATA_DIR = \"/content/drive/MyDrive/MIT_small_train_1/MIT_small_train_{}\"\n",
        "else:\n",
        "    DATA_DIR = \"./MIT_small_train_{}\"\n",
        "\n",
        "# Dataset 1\n",
        "train_loader_1, test_loader_1 = get_dataset(DATA_DIR.format(1))\n",
        "# Dataset 2\n",
        "#train_loader_2, test_loader_2 = get_dataset(DATA_DIR.format(2))\n",
        "# Dataset 3\n",
        "#train_loader_3, test_loader_3 = get_dataset(DATA_DIR.format(3))\n",
        "# Dataset 4\n",
        "#train_loader_4, test_loader_4 = get_dataset(DATA_DIR.format(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "013e69f8",
      "metadata": {
        "id": "013e69f8"
      },
      "source": [
        "## Check existing architectures: ResNet50\n",
        "We are going to try ResNet50 pretrained on ImageNet and evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4aff3e3",
      "metadata": {
        "id": "d4aff3e3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cb86933",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "4cb86933",
        "outputId": "36ab7cd6-0d7a-4a70-87c7-e2d39be1e556"
      },
      "outputs": [],
      "source": [
        "examples = enumerate(train_loader_1)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "n = np.random.randint(low = 0, high = len(example_data)+1, size=1)[0]\n",
        "\n",
        "print(\"N: {}\".format(n))\n",
        "\n",
        "x = preprocess_input_manual(example_data)\n",
        "model = ResNet50(weights='imagenet')\n",
        "preds = model.predict(x)\n",
        "\n",
        "examples = enumerate(train_loader_1)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "print('Predicted:', decode_predictions(preds, top=3)[n])\n",
        "print('Real:', CLASSES[np.argmax(example_targets[n])])\n",
        "\n",
        "plt.imshow(example_data[n], cmap='gray', interpolation='none')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae09bd23",
      "metadata": {
        "id": "ae09bd23"
      },
      "source": [
        "## Fine tuning and existing architecture\n",
        "Now we are we are going to use ResNet50's skeleton and change the last layer to perform the classification. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e617424c",
      "metadata": {
        "id": "e617424c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca6f117",
      "metadata": {
        "id": "fca6f117"
      },
      "outputs": [],
      "source": [
        "def plot_loss_accuracy(history, title):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25,8))\n",
        "    \n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Plot history: CrossEntropy\n",
        "    ax1.plot(history.history['loss'], label='CrossEntropy (training data)')\n",
        "    ax1.plot(history.history['val_loss'], label='CrossEntropy (validation data)')\n",
        "    ax1.set_title('Loss Function: Cross Entropy')\n",
        "    ax1.set(xlabel='Epoch', ylabel='Loss Value')\n",
        "    ax1.legend(loc=\"upper left\")\n",
        "\n",
        "    # Plot history: Accuracy\n",
        "    ax2.plot(history.history['accuracy'], label='Accuracy (training data)')\n",
        "    ax2.plot(history.history['val_accuracy'], label='Accuracy (validation data)')\n",
        "    ax2.set_title('Accuracy')\n",
        "    ax2.set(xlabel='Epoch', ylabel='Accuracy Value')\n",
        "    ax2.legend(loc=\"upper left\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.savefig('accuracy_loss.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0b9e1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e0b9e1c",
        "outputId": "a8e3a13b-48c3-4dbb-c001-5fd1f1248eb8"
      },
      "outputs": [],
      "source": [
        "def buil_custom_resnet50(opti ='adadelta',summary=False):\n",
        "    # ResNet Base Model\n",
        "    base_model = ResNet50(weights='imagenet')\n",
        "    x = base_model.layers[-2].output\n",
        "\n",
        "    # Perform classification\n",
        "    predictions = Dense(len(CLASSES), activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # We don't want to modify imagenet weights\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "            \n",
        "    # Compile model\n",
        "    model.compile(optimizer= opti, loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
        "    if summary:\n",
        "      model.summary()\n",
        "    return model\n",
        "\n",
        "# Training on first datasets\n",
        "model = buil_custom_resnet50()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7Aq7oPUB_h1V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7Aq7oPUB_h1V",
        "outputId": "c567e915-04d6-49fb-e90d-e77aa44da3c4"
      },
      "outputs": [],
      "source": [
        "history_1 = model.fit(train_loader_1,steps_per_epoch= int(400 // BATCH_SIZE), epochs=50, validation_data=test_loader_1, validation_steps= int(validation_samples // BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c83fdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "81c83fdb",
        "outputId": "9be9ba58-87ea-4b1b-e222-15aaef99b270"
      },
      "outputs": [],
      "source": [
        "plot_loss_accuracy(history_1, 'ResNet50: Dataset 1')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d2a012b",
      "metadata": {
        "id": "9d2a012b"
      },
      "source": [
        "With the example code delivered we obtain the above results by using the weights from ResNet50 after training with Imagenet. Now we will start to search for the optimal values of the hyperparameters to get better results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jVJbCMojgBXx",
      "metadata": {
        "id": "jVJbCMojgBXx"
      },
      "source": [
        "We will always use the Early stopping to avoid trainings that are not evolving to give better results. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JSfK-afubi-D",
      "metadata": {
        "id": "JSfK-afubi-D"
      },
      "source": [
        "**Random search**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IIUE_y_UcJNY",
      "metadata": {
        "id": "IIUE_y_UcJNY"
      },
      "source": [
        "To perform the training with different batch sizes we have to load the data witht the desired batch size previously. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6QQ7Kj88bsUa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6QQ7Kj88bsUa",
        "outputId": "59c9d459-245a-44d7-8c8a-4fe3f6c38bd6"
      },
      "outputs": [],
      "source": [
        "from random import random, randint\n",
        "\n",
        "batch_sizes = [randint(10 ,100) for i in range(7)] \n",
        "print('Batch sizes that will be used: ', batch_sizes)\n",
        "\n",
        "optimizers = [tf.keras.optimizers.SGD, tf.keras.optimizers.RMSprop , tf.keras.optimizers.Adam ,tf.keras.optimizers.Adadelta ,\n",
        "              tf.keras.optimizers.Adagrad , tf.keras.optimizers.Adamax , tf.keras.optimizers.Nadam] \n",
        "\n",
        "\n",
        "for BATCH_SIZE in batch_sizes:\n",
        "  train_loader_1, test_loader_1 = get_dataset(DATA_DIR.format(1),BATCH_SIZE)\n",
        "  #After loading the dataset we have to compile the dataset with the desired parameters\n",
        "  #we will get one random value for the learning rate, the optimizer and the momentum used in the optimizer \n",
        "  opti = np.random.randint( 6,  size = 1)\n",
        "  \n",
        "  lr_random = random()#number between 0-1\n",
        "  lr = 0.0001 + (lr_random * (0.3 - 0.0001))#rescaling the number to our range of interest between 0.0001 and 0.3\n",
        "  mom_random = random()#number between 0-1\n",
        "  moment =  (lr_random * (0.9))\n",
        "  if opti<2:\n",
        "    optimizer = optimizers[opti[0]](learning_rate = lr , momentum = moment)\n",
        "  else: \n",
        "    optimizer = optimizers[opti[0]](learning_rate = lr )\n",
        "    moment = 'Not used'\n",
        "\n",
        "  model = buil_custom_resnet50(opti = optimizer,summary=False)\n",
        "  # We will train all the models with 50 epochs and set an early stop if the validation accuracy doesn't get better after 10 epochs \n",
        "  early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "\n",
        "  history_1 = model.fit(train_loader_1,steps_per_epoch= int(400 // BATCH_SIZE), epochs = 50, validation_data=test_loader_1, validation_steps= int(validation_samples // BATCH_SIZE),  callbacks=[early_stop])\n",
        "  plot_loss_accuracy(history_1, 'ResNet50: Batch Size ={}, lr = {}, optimizer = {}, momentum = {}'.format(BATCH_SIZE, round(lr,3),optimizers[opti[0]], moment ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XeOalfUkDtU5",
      "metadata": {
        "id": "XeOalfUkDtU5"
      },
      "source": [
        "After the first one we will perform a second random search to get better result with the range of values adjusted and only ussin SGD. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wZnlLLyID4gy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wZnlLLyID4gy",
        "outputId": "5b1c26f0-16bc-4aa1-8748-3f00b7e8e202"
      },
      "outputs": [],
      "source": [
        "from random import random, randint\n",
        "\n",
        "batch_sizes = [randint(30 ,70) for i in range(5)] \n",
        "print('Batch sizes that will be used: ', batch_sizes)\n",
        "\n",
        "\n",
        "for BATCH_SIZE in batch_sizes:\n",
        "  train_loader_1, test_loader_1 = get_dataset(DATA_DIR.format(1),BATCH_SIZE)\n",
        "  #After loading the dataset we have to compile the dataset with the desired parameters\n",
        "  #we will get one random value for the learning rate, the optimizer and the momentum used in the optimizer \n",
        "  \n",
        "  \n",
        "  lr_random = random()#number between 0-1\n",
        "  lr = 0.04 + (lr_random * (0.1 - 0.04))#rescaling the number to our range of interest between 0.0001 and 0.3\n",
        "  mom_random = random()#number between 0-1\n",
        "  moment =  0.1 + (lr_random * (0.3 - 0.1))\n",
        "  \n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate = lr , momentum = moment)\n",
        " \n",
        "  model = buil_custom_resnet50(opti = optimizer,summary=False)\n",
        "  # We will train all the models with 50 epochs and set an early stop if the validation accuracy doesn't get better after 10 epochs \n",
        "  early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "\n",
        "  history_1 = model.fit(train_loader_1,steps_per_epoch= int(400 // BATCH_SIZE), epochs = 50, validation_data=test_loader_1, validation_steps= int(validation_samples // BATCH_SIZE),  callbacks=[early_stop])\n",
        "  plot_loss_accuracy(history_1, 'ResNet50: Batch Size ={}, lr = {}, optimizer = {}, momentum = {}'.format(BATCH_SIZE, round(lr,3), optimizer , moment ))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u1kIYzXjk8Oy",
      "metadata": {
        "id": "u1kIYzXjk8Oy"
      },
      "source": [
        "After the previous test we found the hyperparaeters to perform a satisfying classification with the given dataset. Now we will try to enhance it more with data augmentation. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sYKhbY7mlTXa",
      "metadata": {
        "id": "sYKhbY7mlTXa"
      },
      "source": [
        "**Data augmentation** \n",
        "\n",
        "Now we will introduce different transformations to our datset in order to enlarge the number of cases that the neural network is being trained with. After that it will be tested with the test dataset without applying the transformations.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "atJsAwVWk70Q",
      "metadata": {
        "id": "atJsAwVWk70Q"
      },
      "outputs": [],
      "source": [
        "#Loading training and validation with data augmentation\n",
        "BATCH_SIZE = 62\n",
        "path = DATA_DIR.format(1)\n",
        "\n",
        "datagen_train_valdidation = ImageDataGenerator(featurewise_center=False,\n",
        "              samplewise_center=False,\n",
        "              featurewise_std_normalization=False,\n",
        "              samplewise_std_normalization=False,\n",
        "              preprocessing_function=preprocess_input_manual,\n",
        "              rotation_range=0.,\n",
        "              width_shift_range=15,\n",
        "              height_shift_range=15,\n",
        "              shear_range=0.,\n",
        "              zoom_range=0.2,\n",
        "              channel_shift_range=0.,\n",
        "              fill_mode='nearest',\n",
        "              cval=0.,\n",
        "              horizontal_flip=True,\n",
        "              vertical_flip=False,\n",
        "              rescale=None, \n",
        "              validation_split=0.2)\n",
        "\n",
        "train_loader = datagen_train_valdidation.flow_from_directory(\n",
        "            path+'/train',  \n",
        "            target_size=(IMG_SIZE, IMG_SIZE), \n",
        "            batch_size=BATCH_SIZE,\n",
        "            classes = CLASSES,\n",
        "            class_mode='categorical', \n",
        "            subset='training') \n",
        "\n",
        "validation_loader = datagen_train_valdidation.flow_from_directory(\n",
        "          path+'/train',  \n",
        "          target_size=(IMG_SIZE, IMG_SIZE), \n",
        "          batch_size=BATCH_SIZE,\n",
        "          classes = CLASSES,\n",
        "          class_mode='categorical', \n",
        "          subset='validation') \n",
        "\n",
        "#loading test data without data augmentation\n",
        "datagen_test = ImageDataGenerator(featurewise_center=False,\n",
        "          samplewise_center=False,\n",
        "          featurewise_std_normalization=False,\n",
        "          samplewise_std_normalization=False,\n",
        "          preprocessing_function=preprocess_input_manual,\n",
        "          rotation_range=0.,\n",
        "          width_shift_range=0.,\n",
        "          height_shift_range=0.,\n",
        "          shear_range=0.,\n",
        "          zoom_range=0.,\n",
        "          channel_shift_range=0.,\n",
        "          fill_mode='nearest',\n",
        "          cval=0.,\n",
        "          horizontal_flip=False,\n",
        "          vertical_flip=False,\n",
        "          rescale=None)\n",
        "\n",
        "test_loader = datagen_test.flow_from_directory(\n",
        "        path+'/test',\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        classes = CLASSES,\n",
        "        class_mode='categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kBrgwL4_rRdu",
      "metadata": {
        "id": "kBrgwL4_rRdu"
      },
      "outputs": [],
      "source": [
        "# Build the model with the best found hyperparameters and we will also introduce learninf rate decay to see if the accuracy can get better with it \n",
        "lr = 0.045\n",
        "moment = 0.11\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate = lr , momentum = moment)\n",
        "\n",
        "model = buil_custom_resnet50(opti = optimizer,summary=False)\n",
        "# We will train all the models with 50 epochs and set an early stop if the validation accuracy doesn't get better after 15 epochs\n",
        "#and also reduce the learning rate after 5 epochs with no improvement by a factor 0.1\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)\n",
        "reduce_lr =   tf.keras.callbacks.ReduceLROnPlateau(\n",
        "      monitor=\"val_accuracy\",\n",
        "      factor=0.1,\n",
        "      patience=5,\n",
        "      verbose=0,\n",
        "      mode=\"auto\",\n",
        "      min_delta=0,\n",
        "      cooldown=0)\n",
        "\n",
        "history_1 = model.fit(train_loader_1,steps_per_epoch= int(400 // BATCH_SIZE), epochs = 100, validation_data=test_loader_1, validation_steps= int(validation_samples // BATCH_SIZE),  callbacks=[early_stop, reduce_lr])\n",
        "plot_loss_accuracy(history_1, 'ResNet50: Batch Size ={}, lr = {}, optimizer = {}, momentum = {}'.format(BATCH_SIZE, round(lr,3), optimizer , moment ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f76c867",
      "metadata": {},
      "outputs": [],
      "source": [
        "(loss, acc) = model.evaluate(train_loader_1, verbose = 0)\n",
        "print(\"Train Accuracy: {:.4f}  Train Loss: {:.4f}\".format(acc,loss))\n",
        "(loss, acc) = model.evaluate(test_loader_1, verbose = 0)\n",
        "print(\"Test Accuracy: {:.4f}  Test Loss: {:.4f}\".format(acc,loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ccee68",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hyperparameter_searching.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b57afa678a37970e2ae1e97bfae723b2640c4399908e06bd470e86425548b2c3"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
